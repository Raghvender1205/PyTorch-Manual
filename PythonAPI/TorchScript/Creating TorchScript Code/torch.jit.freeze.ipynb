{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TORCH.JIT.FREEZE\n",
    "\n",
    "<b>\n",
    "    \n",
    "```python\n",
    "torch.jit.freeze(mod, preserved_attrs=None, optimize_numerics=True)\n",
    "```\n",
    "<b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Freezing a ```ScriptModule``` will clone it and attempt to inline the cloned module's submodules, parameters and \n",
    "attributes as constant in the TorchScript IR Graph. By default, <i>```forward```</i> will be preserved, as well as attributes & methods specified in ```preserved_attrs```. Additionally, any attribute that is modified within a preserved method will be preserved.\n",
    "\n",
    "Freezing currently only accepts ```ScriptModules``` that are in ```eval``` mode.\n",
    "\n",
    "Freezing applies generic optimization that will speed up your model regardless of machine. To further optimize using server-specific settings, run <b><i>```optimize_for_inference```</i></b> after freezing.\n",
    "\n",
    "\n",
    "### Parameters\n",
    "- <b>mod</b> (```ScriptModule```) := A module to be frozen\n",
    "- <b>preserved_attrs</b> (```Optional[List[str]]```) := A list of attributes to preserve addition to the forward method.\n",
    "- <b>optimize_numerics</b> (```bool```) := If ```True```, a set of optimization passes will be run that does not strictly.\n",
    "\n",
    "### Returns\n",
    "Frozen ```ScriptModule```\n",
    "\n",
    "### Example (Freezing a simple module with a Parameter) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "def forward(self,\n",
      "    input: Tensor) -> Tensor:\n",
      "  output = torch.mm(CONSTANTS.c0, input)\n",
      "  if torch.eq(torch.dim(output), 2):\n",
      "    ret = torch.addmm(CONSTANTS.c1, output, CONSTANTS.c2, beta=1, alpha=1)\n",
      "    output0 = ret\n",
      "  else:\n",
      "    output1 = torch.matmul(output, CONSTANTS.c2)\n",
      "    output2 = torch.add_(output1, CONSTANTS.c1, alpha=1)\n",
      "    output0 = output2\n",
      "  return output0\n",
      "\n",
      "Graph:  graph(%self : __torch__.___torch_mangle_0.MyModule,\n",
      "      %input.1 : Tensor):\n",
      "  %22 : Float(2:1, 3:2, requires_grad=0, device=cpu) = prim::Constant[value= 0.3563 -0.5785  0.5724 -0.3773 -0.4578  0.3807 [ CPUFloatType{2,3} ]]()\n",
      "  %7 : int = prim::Constant[value=2]() # C:\\ProgramData\\Anaconda3\\envs\\ML\\lib\\site-packages\\torch\\nn\\functional.py:1688:22\n",
      "  %6 : int = prim::Constant[value=1]()\n",
      "  %self.linear.bias : Float(3:1, requires_grad=0, device=cpu) = prim::Constant[value=-0.0949  0.2828  0.6798 [ CPUFloatType{3} ]]()\n",
      "  %self.weight : Float(2:3, 3:1, requires_grad=0, device=cpu) = prim::Constant[value=-0.5692  0.2085 -0.6571  0.4824  0.3420  0.5020 [ CPUFloatType{2,3} ]]()\n",
      "  %output.2 : Tensor = aten::mm(%self.weight, %input.1) # <ipython-input-1-47315a89736d>:11:17\n",
      "  %10 : int = aten::dim(%output.2) # C:\\ProgramData\\Anaconda3\\envs\\ML\\lib\\site-packages\\torch\\nn\\functional.py:1688:7\n",
      "  %11 : bool = aten::eq(%10, %7) # C:\\ProgramData\\Anaconda3\\envs\\ML\\lib\\site-packages\\torch\\nn\\functional.py:1688:7\n",
      "  %output.4 : Tensor = prim::If(%11) # C:\\ProgramData\\Anaconda3\\envs\\ML\\lib\\site-packages\\torch\\nn\\functional.py:1688:4\n",
      "    block0():\n",
      "      %ret.1 : Tensor = aten::addmm(%self.linear.bias, %output.2, %22, %6, %6) # C:\\ProgramData\\Anaconda3\\envs\\ML\\lib\\site-packages\\torch\\nn\\functional.py:1690:14\n",
      "      -> (%ret.1)\n",
      "    block1():\n",
      "      %output.1 : Tensor = aten::matmul(%output.2, %22) # C:\\ProgramData\\Anaconda3\\envs\\ML\\lib\\site-packages\\torch\\nn\\functional.py:1692:17\n",
      "      %output.3 : Tensor = aten::add_(%output.1, %self.linear.bias, %6) # C:\\ProgramData\\Anaconda3\\envs\\ML\\lib\\site-packages\\torch\\nn\\functional.py:1694:12\n",
      "      -> (%output.3)\n",
      "  return (%output.4)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "class MyModule(torch.nn.Module):\n",
    "    def __init__(self, N, M):\n",
    "        super(MyModule, self).__init__()\n",
    "        \n",
    "        self.weight = torch.nn.Parameter(torch.randn(N, M))\n",
    "        self.linear = torch.nn.Linear(N, M)\n",
    "    \n",
    "    def forward(self, input):\n",
    "        output = self.weight.mm(input)\n",
    "        output = self.linear(output)\n",
    "        return output\n",
    "    \n",
    "scripted_module = torch.jit.script(MyModule(2, 3).eval())\n",
    "frozen_module = torch.jit.freeze(scripted_module)\n",
    "\n",
    "# parameters have been removed and inlined into the Graph as constants\n",
    "assert len(list(frozen_module.named_parameters())) == 0\n",
    "# See the compiled graph as Python code\n",
    "print(frozen_module.code)\n",
    "print('Graph: ', frozen_module.graph)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example (Freezing a Module with preserved attributes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "super(type, obj): obj must be an instance or subtype of type",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-6a691d3b36e3>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m \u001b[0mscripted_module\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjit\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscript\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mMyModule2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0meval\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m \u001b[0mfrozen_module\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjit\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfreeze\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mscripted_module\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpreserved_attrs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"version\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;31m# we've manually preserved `version`, so it still exists on the frozen module and can be modified\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-4-6a691d3b36e3>\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[0mMyModule2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mModule\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m         \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mMyModule\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: super(type, obj): obj must be an instance or subtype of type"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "class MyModule2(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MyModule, self).__init__()\n",
    "        \n",
    "    def forward(self, input):\n",
    "        self.modified_tensor += 1\n",
    "        return input + self.modified_tensor\n",
    "    \n",
    "\n",
    "scripted_module = torch.jit.script(MyModule2().eval())\n",
    "frozen_module = torch.jit.freeze(scripted_module, preserved_attrs=[\"version\"])\n",
    "# we've manually preserved `version`, so it still exists on the frozen module and can be modified\n",
    "assert frozen_module.version == 1\n",
    "frozen_module.version = 2\n",
    "# `modified_tensor` is detected as being mutated in the forward, so freezing preserves\n",
    "# it to retain model semantics\n",
    "assert frozen_module(torch.tensor(1)) == torch.tensor(12)\n",
    "# now that we've run it once, the next result will be incremented by one\n",
    "assert frozen_module(torch.tensor(1)) == torch.tensor(13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML",
   "language": "python",
   "name": "ml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
