{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction To TorchScript"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.9.0+cu111\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Basics of PyTorch Model Authoring\n",
    "\n",
    "A ```Module``` is the basic unit of composition in PyTorch. It contains:\n",
    "- A constructor, which prepares the module for invocation.\n",
    "- A set of ```Parameters``` and sub-```Modules```. These are initialized by the constructor and can be used by the module using invocation.\n",
    "- A ```forward``` function. This is the code that will run when the module is invoked."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([[0.5077, 0.7341, 0.9227, 0.7825],\n",
      "        [0.6454, 0.5516, 0.7677, 0.7136],\n",
      "        [0.7743, 0.8029, 0.9093, 0.7055]]), tensor([[0.5077, 0.7341, 0.9227, 0.7825],\n",
      "        [0.6454, 0.5516, 0.7677, 0.7136],\n",
      "        [0.7743, 0.8029, 0.9093, 0.7055]]))\n"
     ]
    }
   ],
   "source": [
    "class MyCell(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MyCell, self).__init__()\n",
    "        \n",
    "    def forward(self, x, h):\n",
    "        new_h = torch.tanh(x + h)\n",
    "        return new_h, new_h\n",
    "    \n",
    "my_cell = MyCell()\n",
    "x = torch.rand(3, 4)\n",
    "h = torch.rand(3, 4)\n",
    "print(my_cell(x, h))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So:\n",
    "1. Created a class that subclasses ```torch.nn.Module```\n",
    "2. Defined a Constructor. The Constructor doesn't do much, just calls the constructor for ```super```.\n",
    "3. Defined a ```forward``` function, which takes two inputs and returns two outputs. The actual contents of the ```forward``` function are not important.\n",
    "\n",
    "It is a sort of Fake ```RNN-Cell``` it’s a function that is applied on a loop.\n",
    "\n",
    "We instantiated the module, and made ```x``` and ```h```, which are just 3x4 matrices of random values. Then we invoked the cell with ```my_cell(x, h)```. This in turn calls our ```forward``` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MyCell(\n",
      "  (linear): Linear(in_features=4, out_features=4, bias=True)\n",
      ")\n",
      "(tensor([[ 0.0566,  0.7795,  0.8494,  0.8585],\n",
      "        [-0.0117,  0.5529,  0.8503,  0.8103],\n",
      "        [-0.3661,  0.1831,  0.9443,  0.8218]], grad_fn=<TanhBackward>), tensor([[ 0.0566,  0.7795,  0.8494,  0.8585],\n",
      "        [-0.0117,  0.5529,  0.8503,  0.8103],\n",
      "        [-0.3661,  0.1831,  0.9443,  0.8218]], grad_fn=<TanhBackward>))\n"
     ]
    }
   ],
   "source": [
    "# A little something more....!!\n",
    "class MyCell(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MyCell, self).__init__()\n",
    "        self.linear = torch.nn.Linear(4, 4)\n",
    "    \n",
    "    def forward(self, x, h):\n",
    "        new_h = torch.tanh(self.linear(x) + h)\n",
    "        return new_h, new_h\n",
    "    \n",
    "my_cell = MyCell()\n",
    "print(my_cell)\n",
    "print(my_cell(x, h))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We’ve redefined the module ```MyCell```, but this time a ```self.linear``` attribute is added, and we invoke ```self.linear``` in the forward function.\n",
    "\n",
    "```torch.nn.Linear``` is a Module from the PyTorch standard library. Just like ```MyCell```, it can be invoked using the call syntax. We are building a hierarchy of ```Module```s.\n",
    "\n",
    "```print``` on a ```Module``` will give a visual representation of the ```Module```’s subclass hierarchy. In our example, we can see our ```Linear``` subclass and its parameters.\n",
    "\n",
    "You may have noticed ```grad_fn``` on the outputs. This is a detail of PyTorch’s method of <b>automatic differentiation</b>, called [autograd](https://pytorch.org/tutorials/beginner/blitz/autograd_tutorial.html). In short, this system allows us to compute derivatives through potentially complex programs. The design allows for a massive amount of flexibility in model authoring.\n",
    "\n",
    "### Examine Flexibity:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MyCell(\n",
      "  (dg): DecisionGate()\n",
      "  (linear): Linear(in_features=4, out_features=4, bias=True)\n",
      ")\n",
      "(tensor([[ 0.8774,  0.6067,  0.8555, -0.2656],\n",
      "        [ 0.8366,  0.4300,  0.8122, -0.0296],\n",
      "        [ 0.8353,  0.1606,  0.9389, -0.6636]], grad_fn=<TanhBackward>), tensor([[ 0.8774,  0.6067,  0.8555, -0.2656],\n",
      "        [ 0.8366,  0.4300,  0.8122, -0.0296],\n",
      "        [ 0.8353,  0.1606,  0.9389, -0.6636]], grad_fn=<TanhBackward>))\n"
     ]
    }
   ],
   "source": [
    "class DecisionGate(torch.nn.Module):\n",
    "    def forward(self, x):\n",
    "        if x.sum() > 0:\n",
    "            return x\n",
    "        else:\n",
    "            return -x\n",
    "    \n",
    "class MyCell(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MyCell, self).__init__()\n",
    "        self.dg = DecisionGate()\n",
    "        self.linear = torch.nn.Linear(4, 4)\n",
    "        \n",
    "    def forward(self, x, h):\n",
    "        new_h = torch.tanh(self.dg(self.linear(x)) + h)\n",
    "        return new_h, new_h\n",
    "\n",
    "my_cell = MyCell()\n",
    "print(my_cell)\n",
    "print(my_cell(x, h))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once again redefined the ```MyCell``` class, also we have defined ```DecisionGate```. This module utilizes <b>control flow</b>. Control flow consists of things like ```loops``` and ```if``` statements.\n",
    "\n",
    "Many frameworks take the approach of computing symbolic derivatives given a full program representation.\n",
    "\n",
    "But <b>```PyTorch```</b> and <b>```TensorFlow```</b> use ```Gradient Tape```. First, record operations as they occur, and replay them backwards in computing derivatives. \n",
    "\n",
    "In this way, the framework does not have to explicitly define derivatives for all constructs in the language.\n",
    "\n",
    "Working of ```autograd```\n",
    "\n",
    "<img src=\"https://github.com/pytorch/pytorch/raw/master/docs/source/_static/img/dynamic_graph.gif\"></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Basics of TorchScript\n",
    "\n",
    "TorchScript provides tools to capture the definition of the model, even in light of flexible and dynamic nature of PyTorch.\n",
    "\n",
    "### Tracing ```Modules``` \n",
    "\n",
    "Tracing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MyCell(\n",
      "  original_name=MyCell\n",
      "  (linear): Linear(original_name=Linear)\n",
      ")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([[0.7824, 0.4234, 0.7828, 0.6842],\n",
       "         [0.9050, 0.3459, 0.7464, 0.2836],\n",
       "         [0.7555, 0.6908, 0.8771, 0.8293]], grad_fn=<TanhBackward>),\n",
       " tensor([[0.7824, 0.4234, 0.7828, 0.6842],\n",
       "         [0.9050, 0.3459, 0.7464, 0.2836],\n",
       "         [0.7555, 0.6908, 0.8771, 0.8293]], grad_fn=<TanhBackward>))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class MyCell(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MyCell, self).__init__()\n",
    "        self.linear = torch.nn.Linear(4, 4)\n",
    "    \n",
    "    def forward(self, x, h):\n",
    "        new_h = torch.tanh(self.linear(x) + h)\n",
    "        return new_h, new_h\n",
    "    \n",
    "    \n",
    "my_cell = MyCell()\n",
    "x, h = torch.rand(3, 4), torch.rand(3, 4)\n",
    "traced_cell = torch.jit.trace(my_cell, (x, h))\n",
    "print(traced_cell)\n",
    "traced_cell(x, h)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>What exactly has ```torch.jit.trace``` done...??</b>\n",
    "\n",
    "It has invoked a ```Module```, recorded the operations that occurred when the ```Module``` was run, and created an instance of ```torch.jit.ScriptModule``` (```TracedModule``` is an instance of this)\n",
    "\n",
    "TorchScript records its definitions in an ```Intermediate Representation``` (IR) referred to in Deep Learning as a <i>```graph```</i>.\n",
    "\n",
    "We can examine the <i>graph</i> with the ```.graph``` property"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "graph(%self.1 : __torch__.MyCell,\n",
      "      %x : Float(3, 4, strides=[4, 1], requires_grad=0, device=cpu),\n",
      "      %h : Float(3, 4, strides=[4, 1], requires_grad=0, device=cpu)):\n",
      "  %18 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name=\"linear\"](%self.1)\n",
      "  %20 : Tensor = prim::CallMethod[name=\"forward\"](%18, %x)\n",
      "  %11 : int = prim::Constant[value=1]() # <ipython-input-6-171edff523ea>:7:0\n",
      "  %12 : Float(3, 4, strides=[4, 1], requires_grad=1, device=cpu) = aten::add(%20, %h, %11) # <ipython-input-6-171edff523ea>:7:0\n",
      "  %13 : Float(3, 4, strides=[4, 1], requires_grad=1, device=cpu) = aten::tanh(%12) # <ipython-input-6-171edff523ea>:7:0\n",
      "  %14 : (Float(3, 4, strides=[4, 1], requires_grad=1, device=cpu), Float(3, 4, strides=[4, 1], requires_grad=1, device=cpu)) = prim::TupleConstruct(%13, %13)\n",
      "  return (%14)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(traced_cell.graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "def forward(self,\n",
      "    x: Tensor,\n",
      "    h: Tensor) -> Tuple[Tensor, Tensor]:\n",
      "  _0 = torch.add((self.linear).forward(x, ), h)\n",
      "  _1 = torch.tanh(_0)\n",
      "  return (_1, _1)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# We can also print out Python-syntax interpretation of the code for end users.\n",
    "print(traced_cell.code)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So why did we do all this? There are several reasons:\n",
    "\n",
    "1. TorchScript code can be invoked in its own interpreter, which is basically a restricted Python Interpreter. This Interpreter does not acquire the <b>Global Interpreter Lock</b>, and so many requests can be processed on the same instance simultanously.\n",
    "\n",
    "2. This format allows us to <b>save the whole model to disk and load it into another environment</b>, such as in server written in a language other than Python.\n",
    "\n",
    "3. TorchScript gives a representation in which we can do <b>compiler optimizations</b> on the code to provide more efficient execution.\n",
    "\n",
    "4. TorchScript allows us to interfere with many <b>backend/device runtimes</b> that require a broader view of the program than individual operators.\n",
    "\n",
    "We can see that invoking ```traced_cell``` produces the same results as the Python Code./"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([[0.7824, 0.4234, 0.7828, 0.6842],\n",
      "        [0.9050, 0.3459, 0.7464, 0.2836],\n",
      "        [0.7555, 0.6908, 0.8771, 0.8293]], grad_fn=<TanhBackward>), tensor([[0.7824, 0.4234, 0.7828, 0.6842],\n",
      "        [0.9050, 0.3459, 0.7464, 0.2836],\n",
      "        [0.7555, 0.6908, 0.8771, 0.8293]], grad_fn=<TanhBackward>))\n",
      "(tensor([[0.7824, 0.4234, 0.7828, 0.6842],\n",
      "        [0.9050, 0.3459, 0.7464, 0.2836],\n",
      "        [0.7555, 0.6908, 0.8771, 0.8293]], grad_fn=<TanhBackward>), tensor([[0.7824, 0.4234, 0.7828, 0.6842],\n",
      "        [0.9050, 0.3459, 0.7464, 0.2836],\n",
      "        [0.7555, 0.6908, 0.8771, 0.8293]], grad_fn=<TanhBackward>))\n"
     ]
    }
   ],
   "source": [
    "print(my_cell(x, h))\n",
    "print(traced_cell(x, h))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Using Scripting to Convert Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-12-f2a457493ea5>:3: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if x.sum() > 0:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "def forward(self,\n",
      "    argument_1: Tensor) -> NoneType:\n",
      "  return None\n",
      "\n",
      "def forward(self,\n",
      "    x: Tensor,\n",
      "    h: Tensor) -> Tuple[Tensor, Tensor]:\n",
      "  _0 = self.dg\n",
      "  _1 = (self.linear).forward(x, )\n",
      "  _2 = (_0).forward(_1, )\n",
      "  _3 = torch.tanh(torch.add(_1, h))\n",
      "  return (_3, _3)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "class DecisionGate(torch.nn.Module):\n",
    "    def forward(self, x):\n",
    "        if x.sum() > 0:\n",
    "            return x\n",
    "        else:\n",
    "            return -x\n",
    "    \n",
    "\n",
    "class MyCell(torch.nn.Module):\n",
    "    def __init__(self, dg):\n",
    "        super(MyCell, self).__init__()\n",
    "        self.dg = dg\n",
    "        self.linear = torch.nn.Linear(4, 4)\n",
    "    \n",
    "    def forward(self, x, h):\n",
    "        new_h = torch.tanh(self.dg(self.linear(x)) + h)\n",
    "        return new_h, new_h\n",
    "    \n",
    "my_cell = MyCell(DecisionGate())\n",
    "traced_cell = torch.jit.trace(my_cell, (x, h))\n",
    "\n",
    "print(traced_cell.dg.code)\n",
    "print(traced_cell.code)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the ```.code``` output, we can see that the ```if-else``` branch is nowhere found...!!\n",
    "\n",
    "```Tracing``` just runs the code, record the operations <i><b>that happen</b></i> and construct a ```ScriptModule``` and things like <b>Control Flow</b> are erased.\n",
    "\n",
    "A <b>```script Compiler```</b>, which does the direct analysis of the Python source code to transform it into TorchScript. Let's Convert ```DecisionGate``` using the script compiler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "def forward(self,\n",
      "    x: Tensor) -> Tensor:\n",
      "  if bool(torch.gt(torch.sum(x), 0)):\n",
      "    _0 = x\n",
      "  else:\n",
      "    _0 = torch.neg(x)\n",
      "  return _0\n",
      "\n",
      "def forward(self,\n",
      "    x: Tensor,\n",
      "    h: Tensor) -> Tuple[Tensor, Tensor]:\n",
      "  _0 = (self.dg).forward((self.linear).forward(x, ), )\n",
      "  new_h = torch.tanh(torch.add(_0, h))\n",
      "  return (new_h, new_h)\n",
      "\n",
      "Script Gate Graph:  graph(%self : __torch__.___torch_mangle_7.DecisionGate,\n",
      "      %x.1 : Tensor):\n",
      "  %3 : NoneType = prim::Constant()\n",
      "  %5 : int = prim::Constant[value=0]() # <ipython-input-12-f2a457493ea5>:3:21\n",
      "  %4 : Tensor = aten::sum(%x.1, %3) # <ipython-input-12-f2a457493ea5>:3:11\n",
      "  %6 : Tensor = aten::gt(%4, %5) # <ipython-input-12-f2a457493ea5>:3:11\n",
      "  %8 : bool = aten::Bool(%6) # <ipython-input-12-f2a457493ea5>:3:11\n",
      "  %20 : Tensor = prim::If(%8) # <ipython-input-12-f2a457493ea5>:3:8\n",
      "    block0():\n",
      "      -> (%x.1)\n",
      "    block1():\n",
      "      %11 : Tensor = aten::neg(%x.1) # <ipython-input-12-f2a457493ea5>:6:19\n",
      "      -> (%11)\n",
      "  return (%20)\n",
      "\n",
      "Script Cell Graph:  graph(%self : __torch__.___torch_mangle_9.MyCell,\n",
      "      %x.1 : Tensor,\n",
      "      %h.1 : Tensor):\n",
      "  %9 : int = prim::Constant[value=1]()\n",
      "  %3 : __torch__.___torch_mangle_7.DecisionGate = prim::GetAttr[name=\"dg\"](%self)\n",
      "  %4 : __torch__.torch.nn.modules.linear.___torch_mangle_8.Linear = prim::GetAttr[name=\"linear\"](%self)\n",
      "  %6 : Tensor = prim::CallMethod[name=\"forward\"](%4, %x.1) # <ipython-input-12-f2a457493ea5>:16:35\n",
      "  %7 : Tensor = prim::CallMethod[name=\"forward\"](%3, %6) # <ipython-input-12-f2a457493ea5>:16:27\n",
      "  %10 : Tensor = aten::add(%7, %h.1, %9) # <ipython-input-12-f2a457493ea5>:16:27\n",
      "  %new_h.1 : Tensor = aten::tanh(%10) # <ipython-input-12-f2a457493ea5>:16:16\n",
      "  %14 : (Tensor, Tensor) = prim::TupleConstruct(%new_h.1, %new_h.1)\n",
      "  return (%14)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "script_gate = torch.jit.script(DecisionGate())\n",
    "\n",
    "my_cell = MyCell(script_gate)\n",
    "script_cell = torch.jit.script(my_cell)\n",
    "\n",
    "print(script_gate.code)\n",
    "print(script_cell.code)\n",
    "\n",
    "print('Script Gate Graph: ', script_gate.graph)\n",
    "print('Script Cell Graph: ', script_cell.graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[-0.0726,  0.0737,  0.0985,  0.8380],\n",
       "         [-0.1994,  0.8374,  0.3498,  0.6823],\n",
       "         [-0.1749,  0.3584,  0.6277,  0.8168]], grad_fn=<TanhBackward>),\n",
       " tensor([[-0.0726,  0.0737,  0.0985,  0.8380],\n",
       "         [-0.1994,  0.8374,  0.3498,  0.6823],\n",
       "         [-0.1749,  0.3584,  0.6277,  0.8168]], grad_fn=<TanhBackward>))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Some new inputs for the Program\n",
    "x, h = torch.rand(3, 4), torch.rand(3, 4)\n",
    "traced_cell(x, h)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Mixing Scripting and Tracing\n",
    "\n",
    "Some Situations call for using ```tracing``` rather than ```scripting```(e.g. a module has many architectural decisions that are made based on constant Python values that we would like to not appear in TorchScript). \n",
    "\n",
    "In that case, ```scripting ``` can be composed with ```tracing``` , ```torch.jit.script``` will inline the code for the traced module, and ```tracing``` will inline the code for a scripted module\n",
    "\n",
    "### Example (First Case)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "def forward(self,\n",
      "    xs: Tensor) -> Tuple[Tensor, Tensor]:\n",
      "  h = torch.zeros([3, 4])\n",
      "  y = torch.zeros([3, 4])\n",
      "  y0 = y\n",
      "  h0 = h\n",
      "  for i in range(torch.size(xs, 0)):\n",
      "    _0 = (self.cell).forward(torch.select(xs, 0, i), h0, )\n",
      "    y1, h1, = _0\n",
      "    y0, h0 = y1, h1\n",
      "  return (y0, h0)\n",
      "\n",
      "RNN Loop Graph:  graph(%self : __torch__.___torch_mangle_18.RNNLoop,\n",
      "      %xs.1 : Tensor):\n",
      "  %24 : bool = prim::Constant[value=1]() # <ipython-input-19-10d01ab8dd89>:8:8\n",
      "  %6 : NoneType = prim::Constant()\n",
      "  %2 : int = prim::Constant[value=3]() # <ipython-input-19-10d01ab8dd89>:7:27\n",
      "  %3 : int = prim::Constant[value=4]() # <ipython-input-19-10d01ab8dd89>:7:30\n",
      "  %20 : int = prim::Constant[value=0]() # <ipython-input-19-10d01ab8dd89>:8:31\n",
      "  %5 : int[] = prim::ListConstruct(%2, %3)\n",
      "  %h.1 : Tensor = aten::zeros(%5, %6, %6, %6, %6) # <ipython-input-19-10d01ab8dd89>:7:15\n",
      "  %12 : int[] = prim::ListConstruct(%2, %3)\n",
      "  %y.1 : Tensor = aten::zeros(%12, %6, %6, %6, %6) # <ipython-input-19-10d01ab8dd89>:7:34\n",
      "  %21 : int = aten::size(%xs.1, %20) # <ipython-input-19-10d01ab8dd89>:8:23\n",
      "  %y : Tensor, %h : Tensor = prim::Loop(%21, %24, %y.1, %h.1) # <ipython-input-19-10d01ab8dd89>:8:8\n",
      "    block0(%i.1 : int, %y.9 : Tensor, %h.11 : Tensor):\n",
      "      %26 : __torch__.___torch_mangle_15.MyCell = prim::GetAttr[name=\"cell\"](%self)\n",
      "      %32 : Tensor = aten::select(%xs.1, %20, %i.1) # <ipython-input-19-10d01ab8dd89>:9:29\n",
      "      %34 : (Tensor, Tensor) = prim::CallMethod[name=\"forward\"](%26, %32, %h.11) # <ipython-input-19-10d01ab8dd89>:9:19\n",
      "      %y.3 : Tensor, %h.5 : Tensor = prim::TupleUnpack(%34)\n",
      "      -> (%24, %y.3, %h.5)\n",
      "  %41 : (Tensor, Tensor) = prim::TupleConstruct(%y, %h)\n",
      "  return (%41)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "class RNNLoop(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(RNNLoop, self).__init__()\n",
    "        self.cell = torch.jit.trace(MyCell(script_gate), (x, h))\n",
    "    \n",
    "    def forward(self, xs):\n",
    "        h, y = torch.zeros(3, 4), torch.zeros(3, 4)\n",
    "        for i in range(xs.size(0)):\n",
    "            y, h = self.cell(xs[i], h)\n",
    "        return y, h\n",
    "    \n",
    "rnn_loop = torch.jit.script(RNNLoop())\n",
    "print(rnn_loop.code)\n",
    "print('RNN Loop Graph: ', rnn_loop.graph)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example (Second Case)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "def forward(self,\n",
      "    xs: Tensor) -> Tensor:\n",
      "  _0, y, = (self.loop).forward(xs, )\n",
      "  return torch.relu(y)\n",
      "\n",
      "Traced Graph:  graph(%self : __torch__.___torch_mangle_37.WrapRNN,\n",
      "      %xs : Float(10, 3, 4, strides=[12, 4, 1], requires_grad=0, device=cpu)):\n",
      "  %22 : __torch__.___torch_mangle_36.RNNLoop = prim::GetAttr[name=\"loop\"](%self)\n",
      "  %18 : (Tensor, Tensor) = prim::CallMethod[name=\"forward\"](%22, %xs)\n",
      "  %19 : Tensor, %y : Tensor = prim::TupleUnpack(%18)\n",
      "  %21 : Float(3, 4, strides=[4, 1], requires_grad=1, device=cpu) = aten::relu(%y) # <ipython-input-22-f8d7d0e5cdf2>:8:0\n",
      "  return (%21)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "class WrapRNN(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(WrapRNN, self).__init__()\n",
    "        self.loop = torch.jit.script(RNNLoop())\n",
    "    \n",
    "    def forward(self, xs):\n",
    "        y, h = self.loop(xs)\n",
    "        return torch.relu(y)\n",
    "    \n",
    "traced = torch.jit.trace(WrapRNN(), (torch.rand(10, 3, 4)))\n",
    "print(traced.code)\n",
    "print('Traced Graph: ', traced.graph)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Saving and Loading Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "traced.save('wrapped_rnn.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RecursiveScriptModule(\n",
      "  original_name=WrapRNN\n",
      "  (loop): RecursiveScriptModule(\n",
      "    original_name=RNNLoop\n",
      "    (cell): RecursiveScriptModule(\n",
      "      original_name=MyCell\n",
      "      (dg): RecursiveScriptModule(original_name=DecisionGate)\n",
      "      (linear): RecursiveScriptModule(original_name=Linear)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "def forward(self,\n",
      "    xs: Tensor) -> Tensor:\n",
      "  _0, y, = (self.loop).forward(xs, )\n",
      "  return torch.relu(y)\n",
      "\n",
      "graph(%self.1 : __torch__.___torch_mangle_37.WrapRNN,\n",
      "      %xs.1 : Tensor):\n",
      "  %3 : __torch__.___torch_mangle_36.RNNLoop = prim::GetAttr[name=\"loop\"](%self.1)\n",
      "  %5 : (Tensor, Tensor) = prim::CallMethod[name=\"forward\"](%3, %xs.1) # :0:0\n",
      "  %6 : Tensor, %y.1 : Tensor = prim::TupleUnpack(%5)\n",
      "  %9 : Tensor = aten::relu(%y.1) # <ipython-input-22-f8d7d0e5cdf2>:8:0\n",
      "  return (%9)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "loaded = torch.jit.load('wrapped_rnn.pt')\n",
    "\n",
    "print(loaded)\n",
    "print(loaded.code)\n",
    "print(loaded.graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML",
   "language": "python",
   "name": "ml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
